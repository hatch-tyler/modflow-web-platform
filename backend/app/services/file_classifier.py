"""File classification service for categorizing uploaded model files.

Categorizes files into groups based on extension, naming patterns, and content analysis.
Also detects and blocks dangerous file types (executables, scripts, nested archives).
"""

import fnmatch
import os
import re
from pathlib import Path
from typing import Optional

# Extensions that should be blocked during upload (security risk)
BLOCKED_EXTENSIONS = {
    # Executables
    '.exe', '.dll', '.so', '.dylib', '.bin', '.com', '.msi', '.app',
    # Scripts
    '.py', '.bat', '.sh', '.cmd', '.ps1', '.vbs', '.js', '.pl', '.rb', '.bash',
    # Archives (nested - potential zip bomb, hidden content)
    '.zip', '.tar', '.gz', '.7z', '.rar', '.bz2', '.xz', '.tgz',
}

# PEST/pyEMU related file extensions
PEST_EXTENSIONS = {
    '.pst',   # PEST control file
    '.tpl',   # Template file
    '.ins',   # Instruction file
    '.par',   # Parameter file
    '.rei',   # Residuals file
    '.rec',   # Record file
    '.jco',   # Jacobian matrix
    '.jcb',   # Jacobian binary
    '.rst',   # Restart file
    '.sen',   # Sensitivity file
    '.res',   # Residuals (alternative)
    '.rmr',   # Run management record
    '.rns',   # Run status
    '.svd',   # SVD file
}

# PEST-related filename patterns (glob patterns)
PEST_PATTERNS = [
    'pest_run.*',
    'forward_run.py',  # Note: .py blocked, but included for completeness
    'pest_config.json',
    '*.phi.*.csv',
    '*_pest.*',
    'pest_*.*',
    '*.base.par.csv',
    '*.obs.csv',
    '*.par.csv',
]

# MODFLOW core structural files (NAM, discretization, flow properties, solver)
MODEL_CORE_EXTENSIONS = {
    # Name files
    '.nam',
    # Discretization
    '.dis', '.disu', '.disv', '.tdis',
    # Basic package / IC
    '.bas', '.bas6', '.ic',
    # Flow packages
    '.bcf', '.lpf', '.upw', '.npf',
    # Solver packages
    '.pcg', '.gmg', '.sip', '.de4', '.nwt', '.ims', '.sms',
    # Output control
    '.oc',
    # MODFLOW 6 simulation files
    'mfsim.nam',
}

# MODFLOW input files - packages and data required by the model
# These are NOT optional; they're required if referenced in the NAM file
MODEL_INPUT_EXTENSIONS = {
    # Boundary condition packages
    '.wel', '.drn', '.riv', '.ghb', '.chd', '.rch', '.evt', '.ets',
    '.str', '.sfr', '.sfr2', '.lak', '.mnw', '.mnw1', '.mnw2',
    '.uzf', '.sub', '.swt', '.hfb', '.fhb', '.res',
    # Storage
    '.sto',
    # Array/external data files
    '.arr', '.ref', '.dat', '.txt',
    # Zone/multiplier
    '.zon', '.mlt', '.pval',
    # Observation packages
    '.hob', '.drob', '.gbob', '.rvob', '.chob', '.stob',
    # MODFLOW 6 packages
    '.gwf', '.gwt', '.gnc', '.mvr', '.csub', '.buy', '.vsc',
    '.rcha', '.evta', '.maw', '.api', '.tvk', '.tvs',
}

# Model output files (generated by MODFLOW, not input)
MODEL_OUTPUT_EXTENSIONS = {
    # Head/budget output
    '.hds', '.hed', '.cbc', '.bud', '.ucn', '.ddn',
    # List files
    '.lst', '.list',
}

# Folders that indicate observation data
OBSERVATION_FOLDERS = {'observations', 'obs', 'observation_data', 'field_data'}

# Filename patterns for observation CSVs
OBSERVATION_PATTERNS = [
    '*obs*.csv',
    'observations.csv',
    '*_observations.csv',
    'obs_*.csv',
    'head_obs*.csv',
    'field_*.csv',
    '*_measured.csv',
    '*calibration*.csv',
]


def is_blocked_file(filename: str) -> bool:
    """
    Check if a file should be blocked based on its extension.

    Args:
        filename: Name or path of the file

    Returns:
        True if the file should be blocked
    """
    ext = Path(filename).suffix.lower()
    return ext in BLOCKED_EXTENSIONS


def get_blocked_files(file_paths: list[str]) -> list[str]:
    """
    Get list of blocked files from a list of file paths.

    Args:
        file_paths: List of file paths to check

    Returns:
        List of blocked file paths
    """
    return [f for f in file_paths if is_blocked_file(f)]


def classify_file(
    file_path: str,
    required_files: Optional[set[str]] = None,
) -> str:
    """
    Classify a single file into a category.

    Args:
        file_path: Path to the file (relative or absolute)
        required_files: Optional set of files FloPy identified as required

    Returns:
        Category string: 'model_core', 'model_input', 'model_output', 'pest',
                        'observation', 'blocked', or 'other'
    """
    path = Path(file_path)
    filename = path.name.lower()
    ext = path.suffix.lower()

    # Check if blocked
    if ext in BLOCKED_EXTENSIONS:
        return 'blocked'

    # Check if in observation folder
    parts = [p.lower() for p in path.parts]
    if any(folder in parts for folder in OBSERVATION_FOLDERS):
        if ext == '.csv':
            return 'observation'

    # Check observation filename patterns
    if ext == '.csv':
        for pattern in OBSERVATION_PATTERNS:
            if fnmatch.fnmatch(filename, pattern):
                return 'observation'

    # Check PEST files
    if ext in PEST_EXTENSIONS:
        return 'pest'

    for pattern in PEST_PATTERNS:
        if fnmatch.fnmatch(filename, pattern):
            return 'pest'

    # Check if file was identified as required by FloPy
    if required_files and file_path in required_files:
        return 'model_core'

    # Check if it's a core MODFLOW structural file
    if ext in MODEL_CORE_EXTENSIONS or filename in MODEL_CORE_EXTENSIONS:
        return 'model_core'

    # Check if it's a model input file (packages, arrays)
    if ext in MODEL_INPUT_EXTENSIONS:
        return 'model_input'

    # Check if it's a model output file
    if ext in MODEL_OUTPUT_EXTENSIONS:
        return 'model_output'

    # Check for common MODFLOW data file patterns (arrays, external files)
    if re.match(r'^[a-z_]+\d*\.(dat|txt|arr|ref)$', filename):
        return 'model_input'

    return 'other'


def classify_files(
    file_paths: list[str],
    required_files: Optional[set[str]] = None,
) -> dict[str, list[dict]]:
    """
    Classify multiple files into categories.

    Args:
        file_paths: List of file paths to classify
        required_files: Optional set of files FloPy identified as required

    Returns:
        Dictionary with category keys and lists of file info dicts
    """
    categories: dict[str, list[dict]] = {
        'model_core': [],
        'model_input': [],
        'model_output': [],
        'pest': [],
        'observation': [],
        'blocked': [],
        'other': [],
    }

    for file_path in file_paths:
        category = classify_file(file_path, required_files)

        # Get file info
        path = Path(file_path)
        file_info = {
            'path': file_path,
            'name': path.name,
            'extension': path.suffix.lower(),
        }

        # Add description for known file types
        file_info['description'] = get_file_description(path.name, path.suffix.lower())

        categories[category].append(file_info)

    return categories


def classify_directory(
    directory: Path,
    required_files: Optional[set[str]] = None,
) -> dict[str, list[dict]]:
    """
    Classify all files in a directory.

    Args:
        directory: Directory path to scan
        required_files: Optional set of files FloPy identified as required

    Returns:
        Dictionary with category keys and lists of file info dicts with sizes
    """
    file_paths = []
    file_sizes = {}

    for root, dirs, files in os.walk(directory):
        for filename in files:
            full_path = Path(root) / filename
            rel_path = full_path.relative_to(directory)
            path_str = str(rel_path).replace('\\', '/')
            file_paths.append(path_str)
            try:
                file_sizes[path_str] = full_path.stat().st_size
            except OSError:
                file_sizes[path_str] = 0

    categories = classify_files(file_paths, required_files)

    # Add sizes to file info
    for category in categories.values():
        for file_info in category:
            file_info['size'] = file_sizes.get(file_info['path'], 0)

    return categories


def detect_observation_csv(file_path: Path) -> Optional[dict]:
    """
    Detect if a CSV file contains observation data and analyze its format.

    Args:
        file_path: Path to the CSV file

    Returns:
        Dict with format info if observations detected, None otherwise
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            header_line = f.readline().strip()
            if not header_line:
                return None

            headers = [h.strip().lower() for h in header_line.split(',')]

            # Check for long format
            if headers[0] == 'wellname':
                # Long format: WellName,Layer,Row,Col,Time,Head or WellName,Layer,Node,Time,Head
                required_long = {'wellname', 'layer', 'time'}
                location_long = {'row', 'col'} if 'row' in headers else {'node'}
                value_col = 'head' if 'head' in headers else headers[-1]

                if required_long.issubset(set(headers)):
                    # Count rows for observation count
                    n_obs = sum(1 for _ in f if _.strip())

                    return {
                        'format': 'long',
                        'column_mapping': {
                            'well_name': 'wellname',
                            'layer': 'layer',
                            'row': 'row' if 'row' in headers else None,
                            'col': 'col' if 'col' in headers else None,
                            'node': 'node' if 'node' in headers else None,
                            'time': 'time',
                            'value': value_col,
                        },
                        'n_observations': n_obs,
                    }

            # Check for wide format
            elif headers[0] == 'time':
                # Wide format: Time,MW-01,MW-02,...
                wells = [h for h in headers[1:] if h]

                # Count observations
                n_obs = 0
                for line in f:
                    if line.strip():
                        parts = line.split(',')
                        n_obs += sum(1 for p in parts[1:] if p.strip())

                return {
                    'format': 'wide',
                    'wells': wells,
                    'n_observations': n_obs,
                }

            return None

    except Exception:
        return None


def get_file_description(filename: str, extension: str) -> str:
    """
    Get a human-readable description for a file based on its name/extension.
    """
    filename_lower = filename.lower()

    # Name files
    if filename_lower == 'mfsim.nam':
        return 'MODFLOW 6 simulation name file'
    if extension == '.nam':
        return 'Model name file'

    # Discretization
    descriptions = {
        '.dis': 'Discretization package',
        '.disu': 'Unstructured discretization',
        '.disv': 'Vertex discretization',
        '.tdis': 'Temporal discretization',
        '.bas': 'Basic package',
        '.bas6': 'Basic package',
        '.ic': 'Initial conditions',

        # Flow packages
        '.bcf': 'Block-centered flow package',
        '.lpf': 'Layer-property flow package',
        '.upw': 'Upstream weighting package',
        '.npf': 'Node property flow package',

        # Solvers
        '.pcg': 'PCG solver',
        '.gmg': 'GMG solver',
        '.sip': 'SIP solver',
        '.de4': 'DE4 solver',
        '.nwt': 'Newton solver',
        '.ims': 'Iterative model solution',
        '.sms': 'Sparse matrix solver',

        # Output
        '.oc': 'Output control',
        '.hds': 'Head output file',
        '.cbc': 'Cell budget output',
        '.lst': 'List file',

        # Boundary conditions
        '.wel': 'Well package',
        '.drn': 'Drain package',
        '.riv': 'River package',
        '.ghb': 'General head boundary',
        '.chd': 'Constant head',
        '.rch': 'Recharge package',
        '.evt': 'Evapotranspiration',
        '.sfr': 'Streamflow routing',
        '.lak': 'Lake package',
        '.uzf': 'Unsaturated zone flow',

        # PEST files
        '.pst': 'PEST control file',
        '.tpl': 'Template file',
        '.ins': 'Instruction file',
        '.par': 'Parameter file',
        '.rei': 'Residuals file',
        '.rec': 'Record file',
        '.jco': 'Jacobian matrix',
    }

    return descriptions.get(extension, '')


def get_categorized_summary(categories: dict[str, list[dict]]) -> dict:
    """
    Generate summary statistics from categorized files.

    Args:
        categories: Output from classify_files or classify_directory

    Returns:
        Summary dict with counts and total sizes
    """
    total_files = 0
    total_size = 0
    category_counts = {}

    for cat_name, files in categories.items():
        count = len(files)
        size = sum(f.get('size', 0) for f in files)
        category_counts[cat_name] = {
            'count': count,
            'size': size,
        }
        total_files += count
        total_size += size

    return {
        'total_files': total_files,
        'total_size_bytes': total_size,
        'total_size_mb': round(total_size / (1024 * 1024), 2),
        'categories': category_counts,
    }
